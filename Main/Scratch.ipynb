{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Absorbing States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 21:23:40.141822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LTL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdependencies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mNN\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdependencies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLTL\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdependencies\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mUtility_funcs\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcsrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmdp\u001b[39;00m \u001b[39mimport\u001b[39;00m GridMDP\n",
      "File \u001b[0;32m~/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/dependencies/Utility_funcs.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mLTL\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_predicates\u001b[39m(grid_mdp):\n\u001b[1;32m      5\u001b[0m         aps \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39munique(grid_mdp\u001b[39m.\u001b[39mlabel))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'LTL'"
     ]
    }
   ],
   "source": [
    "from dependencies.NN import *\n",
    "from dependencies.LTL import *\n",
    "from dependencies.Utility_funcs import *\n",
    "\n",
    "%matplotlib inline\n",
    "from dependencies.csrl.mdp import GridMDP\n",
    "from dependencies.csrl.oa import OmegaAutomaton\n",
    "from dependencies.csrl import ControlSynthesis\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 3)]            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                320       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 288)               4896      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 3, 3, 8)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,744\n",
      "Trainable params: 5,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 08:33:24.157096: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 08:33:24.159098: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "input_gws = []\n",
    "policies = []\n",
    "\n",
    "# policy_model = build_policy_model(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).shape, Policy[0,:,:,:].shape)\n",
    "policy_model = build_policy_model((3,3), (4, 3, 3, 8))\n",
    "policy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Omega-automaton states (including the trap state): 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<csrl.oa.OmegaAutomaton at 0x7f005aff6dc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAGrCAYAAABg2IjeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiTElEQVR4nO3dfWxU14H38d+9M2Njg8fjFyDhPU4CNGQDIdBsk91NU0i0Vbft00dAtPvHtlWURNqoKzWtwqL+0aj/oCL1SZ82rpTyx9I+UquA1W6KtlstZkMaCbJhcfPeQsMkwTZNwDZ+w2bsO/c8fwzjYvDLjH1n5szc76dCscd37j3Tc2Z+97zcO44xxggAAAu4pS4AAABZhBIAwBqEEgDAGoQSAMAahBIAwBqEEgDAGoQSAMAahBIAwBqEEgDAGtFSF6DSJZNJtbW1qaWlRclkUo8//rgSiUSpi4Ui6ujo0GOPPaZTp06VuigogY6ODrW3t0uSTp48qf379/MZMANCqcB27tw58WGUTCb12GOP6dChQyUuFYole0LS0dFR6qKgRNrb2/X0009Lkvbt26dt27ZxgjIDh3vfFU4ymZwUSpLU0NCgS5culbBUKAXHccRbLXw6Ojq0bdu2ifd8MpnUrbfeqrNnz6qlpaXEpbMTc0oF1N7ersbGxkmPNTY2ctYMhMTmzZu1f//+id/7+/sl6YbPBfwZoVRA2QZ4vb6+vuIWBEDJ7NixY+LnF154Qdu3b2dOaQbMKZXAdGEFoHL19/erra2N+aRZ0FMqoEQicUOvqK+vj7MkIIR2796tI0eO8P6fBaFUQNu3b5/y8S1bthS5JABKad++fdq9e7daWlrU39/PaMkMCKUCun51TTKZ1JYtWzhTCik+iMKpra1NmzdvngikgwcP8hkwA5aEF1gymdTzzz+vrVu36uTJk9qzZw8NMkTa29t15MgR7du3T08//bS2bt06aeIblS27BPxaiUSCy0JmQCgBAKzB8B0AwBqEEgDAGoQSAMAahBIAwBqEEgDAGoQSAMAahFKRpFIpPfPMM0qlUqUuCkqA+g836j93XKdUJIODg6qvr9fAwIDi8Xipi4Mio/7DjfrPXVF7Sq2trcU8HCxD/Ycb9Y9c2gChhKKh/sON+od1oQQAwEwK/iV/ra2tE+l43333qbOzU64bviz0PE8/+9nPdOnSJQ0NDZW6OEXn+76+853vUP/UP/UfwvqXMm3gySefnHW7oi506Ozs1KJFixSN8oW3YeN5noaHh6n/kKL+4XmeGhoaZt2uqK3DdV1Fo1EaZUhR/+FG/SMX4etHAwCsRSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKxBKAEArEEoAQCsQSgBAKwRzWfjZDKptrY2tbS0KJlM6vHHH1cikShQ0QAAYZNXKO3cuVOnTp2SlAmoxx57TIcOHSpIwQAA4ZPz8F0ymZz0e0tLi9rb2wMvEAAgvHIOpfb2djU2Nk56rLGxUR0dHYEXCgAQTjkP3/X390/5eF9f35SPp1IppVKpGx4DAGA68159N11Y7d27V/X19ZP+Pffcc/M9HACgguXcU0okEjf0ivr6+qZdfbdnzx499dRTkx7r6enJv4QAgNDIuae0ffv2KR/fsmXLlI9XV1crHo9P+lddXT23UgIAQiHnUGppaZn0ezKZ1JYtW7hOCQAQmLyuUzp06JB2796trVu36uTJk1yjNAPf+Oo3/RowA/LkKW3ScuQo6kRVpSo1uU2qdWpLXUwABTJmxtRrejVqRuXJk298uY6rqKKqdWrV5DQp5sRKXUzrOMYYU6yDdXd3Kx6PKxrNKwvLwhVzRef8c7roX9QF/4J6Ta98+TM+Z4EWaLG7WEvcJVrqLNUyd5lcpzLv/OR5ngYHByu2/jGzMNR/n9+nLr9LPX6PLvgXNKjBWZ8Td+Ja4izREneJVrgrlHAThS9oiXiep7q6ulm3q8zWUSTGGF00F/WO947O+mfly5crd9YwyrqiK+r0O9Xld8nIaKEWakN0g9ZF1qnGqSlw6QHMV9qk9b7/vt723tYFc0GS5MiRUW7n+oNmUENmSO/570mSbnJu0p3RO7XGXVOxJ6izIZTmwBijP/p/1FveW+o1vZMaYa6BNGl/V597WZf1mveaTnondat7qzZFN6nRbZzl2QCK7Yq5oje9N/X79O+VUkqOnIm/5RpIU23/sflYH41/pAVaoDsid+iu6F2qcqoCK3c5IJTyNOgP6tj4MX1kPpp4LN9GOBsjo7P+WZ0dO6vN0c26O3J3aM+aANt8kP5Avx3/rVJKTbz3g/oMyO7niq7od+nf6Q/pP+jTsU9rRWRFIPsvB4RSjowxejf9rl71Xp1Tbyjv411tnKe8U0qmk/pM7DNqcpsKflwAU7tiruj4+PGJobZCMzIa0Yh+Pf5rrU+v11/G/jIUvSZCKQejZlRHxo5M6h0VU7/p1y/GfqGt0a3aGNkox3FmfxKAwHSnu3V0/KhSKs2t0k77p3UudU7bq7brJvemkpShWBgTmsWwGdaLYy/qY/Nxycpgrv7vNe81nfBOqIgLJoHQez/9vn49/mtd0ZXAh+pzle01/fvYv6sr3VWSMhQLoTSDYTOsf0v9m4bMUMka4/XeTr+t347/lmACiuBs+qyOjB+x5v2fVlr/Mf4f+jD9YamLUjCE0jRGzagOjx3WqEataZBZp/3TetV7lWACCuhc+pz+a/y/Sl2MGxgZHRk/ovPp86UuSkEQSlPwja/fjP1Gw2bYukDKeiv9ln6f/n2piwFUpD6/T/85/p/Wvv99+frN+G806M9+gW65IZSm8Gb6TV00F61tkFknvBMV2SiBUvKNr5fGX7L+/Z9WWsfGj1XciAmhdJ1L/iWd9E6Wuhg58eVXZKMESunN9JvqNb3Wh5KR0UfmI72bfrfURQkUoXQN3/hWjiFPJ9soGcYDglFOJ6VZr3qvVtSICaF0jbfTb5fFGdL1TngndNlcLnUxgLJmjNGx8WOlLkbefPl6ZfyVUhcjMITSVb7x9ab3ZqmLMSe+fP3eo7cEzMcFc6Es5pKvZ2TUbbrV5/fNvnEZIJSuOuef04hGSl2MOTHK3ALJN4W//RFQqd7x3pl0Y9Vy4sjRu15lzC0RSleVc4OUMjdw/MD/oNTFAMrSqBnVWf9s2fWSsoyMTvunNWbGSl2UeSOUJA34A+o23WXbIKXMmdLb3tulLgZQlk6nT5f1+1/KLBF/L12cm8UWEqEk6T3/vbLuJUl/Xok3bIZLXRSg7JxOny51EQJRCa+DUJJ0wb9Q9mdJWT1+T6mLAJSVcTOuATNQ6mIEotf0lv3cMqGkTChVAkeOLvoXS10MoKz0mt5SFyEwvnxdMpdKXYx5CX0oXTaXS/YdKUEzMhUTsECxVNqJXLm/ntCHUrlX4PUumovcdgjIw0X/YtnPKWe5ctVjynsIP/TfPNtv+uXIqZg5pTGNKaWUFmhBqYsClIU+0xf4+/9Xz/xKXW90aaR/RL0f9Gp0YFTf7/t+oMeYii+/7C+iDX1PyTNexZwlZXnySl0EoGyMm/HA97nq7lVa9+C6iUBqWtMU+DGmM67gX08xhb6nVIkf4J7xVGE5CxRMWunA97npi5skSec6zumNX72htQ+sDfwY0yn3z7TQ95QAoFDOvHxGkrTu0+uKdsxyH/kJfShFK7CzGHUq7zUBhRJRpCD7HRkY0ejAqCRp7aeL11Mq1OspFkLJiVbMIoesSgxaoFBiTqwg+z1zLNNLalrTpNr62oIcYypVqirasQoh9KHU6DRWVCgt0AItcFh5B+Sq2WkuyJDX6WOZW/4Ucz7Jlasmt3iLKgoh9KHU7DaXugiBWuwuLnURgLLS7DYX5MS0FPNJvvyy/0wLfSjVOrWqUU2pixEIR44WO4QSkI9CnMiNDGSuT5KKO58kZXp+5YzJB2Ua5Tn/XOD7HRkYUfuz7Trz8hnVJmo10j+iR77/iFZuXKmjPziqmniN7vvKfYEdz8jQUwLy1OQEP9x1/XzS8QPHdfwnx1WbyMwtbfripkDf+1kRRZRwEoHvt5gIJUlL3aXq9DsD7cIfP3BcB586qM/882f0zZe+OfH4j770I2364iYdfuawmtY0Bd4wCSUgP1EnqganIdAbmWbnk1bctWLiPZ/9HOj5oEffe/B76nyjU488+0hgx5QyvSTXKe8BsPIufUBui9wWaCAd/cFRHXzqoD7/zOf1hWe+MOlvu57dpYNPHZSUabBBceRoubNctU7xVvkAlWJ9ZH2g+8vOJ515+Yx2Pbtr0sln85pmferLn9KJn5xQ5xudgR53XaR481eFQihJqnPqtMpdFcgKnNdffF2HnzmsFRtXaNs/b7vh781rmiduORLkBKiR0Z3ROwPbHxAmayNr5Qb0cXjtfNJX/vUral5z4xxP9rH2Z9sDOaYkxRTTbZHbAttfqRBKV22IbJh3b2lkYEQHvnpAkvTI96fvlhdiArRWtVrprgxsf0CYVDvVut29PZgT01++LikznzTbiWfvh8F8l5MjR+si6yriwnlC6aoV7got0qJ57ePwM4clZRrjyo1TB0R2rLmmvmbKM6i52hDdUPZjyUApbYjO/8RUyu36pJ4PMl8vkb3jw3wZGd0RuSOQfZUan2JXOY6jTdFN89rHiZ+ckCRt/MLGabcpxAV1McUCHxMHwqbZbdbNzs3z7i1l55OyN2WdStcbXZIyJ6fz5cjRKneVEm5i3vuyAaF0jU9EPqGlztI5Ncps2EjS3V+6e9rtCnFB3f2x+1XjVMa1VkApPRB7YF5zSz0f9Ez0fmZ6j2c/B1Zumv+Qe1RR/XXsr+e9H1sQStdwHEcPxh6cU6PserNr4ufphu6kP58hBTGf5MjRSnelbndvn/e+AEhxN657o/fO+fnZ9/eKjdOvrL32BHam3lSu7o/dr4XOwnnvxxaE0nXm2ih73s+MEefSGK+fT7q2keYjqqj+JvY3cpzyvlU9YJMNkQ1zHjHJLlyYqQf0+ouvS8ptIcRMKvWklFCawobIhrzHlrNjw9krtqeSDZ9N/2vTxGOdb3RONNJ8/VXsryrqDAmwQXbEZC5fAdG0OnO5x0xzRdm5589/+/NzK6AygRRTrCJPSgmlKTiOo4erHlbCSeQcTKvuXiUpt8Z47fDe7375uzl14bdEt+j2SGWdIQG2iLtxfbbqs3kP5c/2tecvfP0FSZnFUPMZuosoos9Vfa4iT0oJpWlUO9X6u6q/U51Tl1Mwrf30WtXU10yaW8oaGRjRj770I9U2ZHpR1zbcMy+fybsLf1fkLt0dmX4xBYD5u9m9WQ/HHs5rxGTlxpVqWtM0Mbd0rddffF0nfnJCax9Yq68e+OqcyxVRRH9b9bcVe0sxQmkGNU6Nvlj1RTU4DbM2zNr6Wj3y/UfU+0HvpDmizjc6deArB7Tr2V0T3fXsxbPHDxzXfV/O795390Tu0b3Reyuuyw7YaFVklT4b+6wiiuQcTrv+zy6defnMpFsIvf7i6zrw1QP61Jc/pX/65T/NqSzZIbvPVX1Oy9xlc9pHOXCMMUX7hrvu7m7F43FFo+V11fGYGdNL4y/pQ//DWbc9fey0jv7fo5IyQ3lNa5q0/evbJ7558viB4zr6g6NacdcKNa1puuHeeFNx5MiVq/ui9+kT0U/M78WUiOd5GhwcLMv6x/yVe/1f8C+ofaxdl3U5pwtsO9/onLiYXsp8Fmz/+vYZV+bOJu7E9VDsobL9Ej/P81RXVzfrdoRSjowxOuuf1Svjr8iTV9Rvq13qLNWDsQcVd+NFO2bQyv1DCfNTCfU/bsb1mvea3km/I0dOUT4DssfZGNmoe6L3lPVthHINpfJ9hUXmOI5ui9ymZe4yvTL+ij70Pyxow8z2ju6N3qsNkQ0M1wElFnNiuj92v26J3KJjY8c0rOGCHzPuxPVg7EEtcZcU/Fi2IJTyVOvU6uHYwzrnn9Nb3ls6b84HHk5RRbUusk5/EfmLsu4dAZVombtMO6t36t30u3rHe0fDGg7sMyC7n7gT152RO7U+sr6se0dzEa5XGxDHcbQ6slqrI6vV7/fr3fS7Op0+rXGN5904r92+wWnQnZE7dVvkNsWcWKGKD2CeYk5MG6MbdVfkLnX5XXon/c7Et1fn+xngypUvX44crXZXa0Nkg5a5y0I7OkIozVPCTeg+9z59MvpJdfld6vF7dNFc1AX/glJKzfjceqdeS5wlWuwu1lJ3qZqd5tA2RKAcOY6jlZGVWhlZqWEzrPP+eV30M+//XtMrX/60z40ooianSUvcJWp2m7XcXV6R1x3li1AKSNSJak1kjdZE1kjKLIwY0YgG/AF58pRWWo4cRRVVlVOlBqeB3hBQQRY5i7Q2slZrI5n7WvrGV7/p16gZlSdPvnxFrv6v1qlVvVPP181MgVAqEMdxtFALtTDCmQ8QRq7jqtFpLHUxyg4xDQCwBqEEALAGoQQAsAahBACwRlEXOvi+L8/zinlIWMLzPOo/xKh/5Fr3BQ+l1tZWtba2SpJ27dqlRx99VK5LBy1sfN9XKpW5bov6Dx/qH77vq6GhYdbtinpD1s7OTi1atKhsb8iIufM8T8PDw9R/SFH/8Dwvp1AqautwXVfRaJRGGVLUf7hR/8gF/WgAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANQglAIA1CCUAgDUIJQCANfIKpY6ODt1zzz2FKgsAIORyDqW2tjZJmWACAKAQorluuGPHjkKWAwAA5pQAAPYglAAA1sh5+C5fqVRKqVTqhscAAJhOwXpKe/fuVX19/aR/zz33XKEOBwCoAI4xxuT1BMdRLk+ZqqfU09OjxYsXKxotWAcNlvI8T4ODg4rH49R/CFH/8DxPdXV1s243p9bR39+vRCIx4zbV1dWqrq6e9NjQ0NBcDgcACImch+/a29u1e/duSZmhuex1SwBmYYyU9qV0WvLzGpgAQifv4bv56O7upvseUqEYvjFGzmhK7uVRuZevyLk8InckJee6t5iRZBZUyV9YI39hjczCGvm1C6RI5S6GDUX9Y0YFHb4D8GfOaErRj/sU6emX4/sykuRIzjSne44k58qYnCtjivQOyFEmqPzEInlLm+THF0qOU7TyAzYhlIC58I3c/sFMGA2NyCgTNsr+N4fxB+e6n93+YVX3D8uvism7qVHp5gYpGgm65IDVCCUgT87wqKqSXXKvjE1kTxD9molQGxtX7NzHinVd0Pjqm5VuTtBzQmgQSkCufF/RrguKftQ78VAhoiK7T+MbVb1/Xum+AY3dslyqihXgaIBdKndmFQiQMzyq6rfPKvpRb2ZOqBjHvPpfd+CyFrz5niIXL2VW8gEVjJ4SMAv30pCq3jsnmeKE0fUcScb3VfX+eY2PpuStXMpwHioWPSVgBpHeAVX9sXSBlJU9duyjXsU+OE+PCRWLUAKm4V4aVOxsl6TSBtL1Ihf7FfvwI4IJFYlQAqbgDo2o6r1OSXYFkpQpT/RCn6LnL5a6KEDgCCXgeum0Ymc7Sz5kN5to90U5w6OlLgYQKEIJuE6s82M5Y57VgZRVleySfL/UxQACQygB13AHLyt64VJZBFL2dkXR7gulLgoQGEIJyEr7iiW7crlDkDUcSdE/9TKMh4pBKAFXRXoulc2w3fVi9JZQIQglQJKMUfTjvlKXYk4cSe7AsJzUWKmLAswboQQoswTcvTJWlr2krMiFS6UuAjBvhBIgKfJxX1nNJV0ve+0SK/FQ7gglYNxT5NJgWfeSJMlJ+4pcGip1MYB5IZQQeu7waNkHkpT5XkF3aKTUxQDmhVBC6LmXR8t66C7LkeQOE0oob4QSQs+9XDnX+DijKW7UirJGKCH03MuVMXwnSY4xmWACyhRf8odwG/fkeOmiHrLt2FG98NIRJf/Urf7hISXPd+v5b+zR45//3/Pet5HkXr6idO2C+RcUKAF6Sgg1x/OKfsyWZcv10JZPquXm5Uqe75YkbVl3R3AHSBf/NQFBoaeEcPOLP/+yee16bV67Xi03L1fby0eVWFSnzWvXB7Z/pwSvCQgKPSWEWwkXBRw59Zokacu6TwS3U0csdEBZI5QQbm7p3gIdZ/4gSXpoy73B7dSopK8JmC9aL8LNKd26u/arPaUdD3wm0P2aEr4mYL4IJYSaqYqV5MLZbC8psahOLctWBLZfR5Kprgpsf0CxEUoIt4grs6D4H+LZXtL2ez4Z+L79hSwHR/kilBB6/qKaoveWjvzPf0uSHtoSbCiZaESqigW6T6CYWBKO0PMX1ijSMxD4fvuHhrT3ZwfUfuo1NdbF1Tc0qP3f/JY2r11fkJ6SUea1AOWMUELo+QtrAr/N0I8P/0JPfG+vnv77f9SpH/+/iccf+saTeuLqnRuCnk+SCCWUP0IJoWcW1shEI4Hdbmjfz3+q3c//UN994mt6+u//cdLfnv/GHt36D1+SFPx8kiMp3VAX6D6BYmNOCXAceUsbA5lXajt2VLuf/6E2r11/QyBJUsuyFWpZtlxSsPNJRlJ6YY0MPSWUOUIJkOQtbpj3PvqHhrTzmX+RJO3/5rem3S57v7sge0qOpPTSxsD2B5QKoQRIUlVM6Yb4vHpLu3/8Q0mZG65Ody+79qur7oKeTzIRV+nGeGD7A0qFUAKuSi9tnNeChx8f/qUkaccD26bd5kiBVt15Sxq5vRAqAq0YuMqPL5TXOLfeUrYHJEmPPPjQ9NtdDaWg5pOMJMWi8pY1B7I/oNQIJeAa46tvlqKRvIOp44+nJ36e6WsosrcXCqqn5Egaa1kuRSKB7A8oNUIJuFYsqrFbluU9jHf2fJekmQNpuvmkbFDlyyizQMOvXzSn5wM2IpSA6/gN8byH8RKLMtcHNdZNv9hgqvmk5PkuPX/4F3mXMTtsN75qad7PBWxGKAFTGL9lmUxtdc7BtPXq15lnw2kq2YUQ184nPX/4l9o5w8KIqRhJch2l1q5i2A4Vh1ACphKJKLVujUx1VU7BtP2eTyqxqE4df7xxKK5/aEgPfeNJNcYzvaiWm5dP/K391GvanseX/BlJchyNrVvNhbKoSIQSMJ1YVKk7bpFZMHuPKVFXp/3f/JaS57snrcTrOPMH7XzmX/T8N/bou49/TZLUPzwsKXN/vJlW6l0v20MaW7daft3CPF8MUB4cY0zR7trf3d2teDyuaJRb7oWN53kaHBwsz/r30qo686Hc4dFZF0C0/89/67s//6mk7IKG5drzD19Roi4zrPfjw7/Qd3/+U22+fb1ali3Xd5/4Wk5FMJIUjShVpj2ksq5/BMLzPNXVzX5vRkIJRVH2H0rGKPpRr6JdH0tGgd9VfNrDKnMsrzGeWa4eK8P/71QB9Y95yzWUaB1ALhxH3s3NSifqVJXski5fKXgwTfSOblkmv4FbCCEcCCUgD6amWqk7WjK9pj/1yPHSE72ZQPaf/cFxlG6u1/iKpWXbOwLmgtYO5Otqr8lb2qjIpSFFPu5VZHh0XuGUfa6piim9tFFec4IwQijR6oG5cl2lm+qVbqqXM3JFkZ5+uUMjckeuyLk6VWsc6Yale1cfy/7JLKiSv7BG6aaE/PqFklOsGSvAPoQSEABTu0Deqpuu/mLkjKbkXr4id+SK5Kcl32QSyHUk15W/oEpmYY382gVShCszgCxCCQia48jULlC6doGC+YJ1IDw4RQMAWINQAgBYg1ACAFiDUAIAWINQAgBYo6ir73zfl+d5xTwkLOF5HvUfYtQ/cq37godSa2urWltbJUm7du3So48+KtelgxY2vu8rlUpJEvUfQtQ/fN9XQ0PDrNsV9S7hnZ2dWrRoEXcJDiHP8zQ8PEz9hxT1D8/zcgqlorYO13UVjUZplCFF/Ycb9Y9c0I8GAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWINQAgBYg1ACAFiDUAIAWCOaz8YdHR1qb2+XJJ08eVL79+9XIpEoRLkAACGUVyi1t7fr6aefliTt27dP27Zt06lTpwpSMABA+OQ8fNfR0aG9e/dO/L5jxw51dHQomUwWpGAAgPDJOZQ2b96s/fv3T/ze398vSWpsbAy8UACAcMpr+G7Hjh0TP7/wwgvavn37tHNKqVRKqVTqhscAAJjOnFbf9ff3q62tTYcOHZp2m71796q+vn7Sv+eee27OBQUAVD7HGGPyfdITTzyh3bt3q6WlZdptpuop9fT0aPHixYpG8+qgoQJ4nqfBwUHF43HqP4Sof3iep7q6ulm3y7t17Nu3byKQsvNKUw3hVVdXq7q6etJjQ0ND+R4OABAieQ3ftbW1afPmzROBdPDgQa5TAgAEJueeUjKZ1M6dOyc9lkgk9PjjjwdeKABAOOUcSi0tLZrD9BMAADnj3ncAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrEEoAAGsQSgAAaxBKAABrRIt5MN/35XleMQ8JS3ieR/2HGPWPXOveMcaYQhaktbVVra2tkqQnn3xSTz75ZCEPZ61UKqW9e/dqz549qq6uLnVxUGTUf7hR/7kreCghY3BwUPX19RoYGFA8Hi91cVBk1H+4Uf+5Y04JAGANQgkAYA1CCQBgDUKpSKqrq/Xtb3+bSc6Qov7DjfrPHQsdAADWoKcEALAGoQQAsAahBACwxv8H4TrPNTv4B0cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LTL Specification\n",
    "ltl = '(F G a | F G b) & G !d'\n",
    "\n",
    "# Translate the LTL formula to an LDBA\n",
    "oa = OmegaAutomaton(ltl)\n",
    "print('Number of Omega-automaton states (including the trap state):',oa.shape[1])\n",
    "display(oa)\n",
    "\n",
    "# MDP Description\n",
    "shape = (3,3)\n",
    "# E: Empty, T: Trap, B: Obstacle\n",
    "structure = np.array([\n",
    "['E',  'E',  'E'],\n",
    "['E',  'E',  'E'],\n",
    "['E',  'E',  'E'],\n",
    "])\n",
    "\n",
    "# Labels of the states\n",
    "label = np.array([\n",
    "[('a',),(),('b',)],\n",
    "[(),('d',),    ()],\n",
    "[(),    (),    ()],\n",
    "],dtype=object)\n",
    "# Colors of the labels\n",
    "lcmap={\n",
    "    ('a',):'lightgreen',\n",
    "    ('b',):'lightgreen',\n",
    "    ('d',):'pink'\n",
    "}\n",
    "p = 1\n",
    "grid_mdp = GridMDP(shape=shape,structure=structure,label=label,lcmap=lcmap, p=p, figsize=5)  # Use figsize=4 for smaller figures\n",
    "grid_mdp.plot()\n",
    "\n",
    "# Construct the product MDP\n",
    "csrl = ControlSynthesis(grid_mdp,oa)\n",
    "max_rew = round(csrl.reward.max(), 3)\n",
    "\n",
    "s_vectors = state_vectors(csrl)\n",
    "enc = list(np.unique(grid_mdp.label))\n",
    "enc.pop(enc.index(()))\n",
    "ch_states = channeled(csrl, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',), ('b',), ('d',)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 3.],\n",
       "       [0., 4., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channeled(csrl, enc, agent=False)[0,0,0,0].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [0], 'b': [2], 'd': [4]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_t = \"(<> [] a \\/ <> [] b) /\\ [] ~d\"\n",
    "\n",
    "LTL_formula = parser.parse(full_t)\n",
    "predicates=get_predicates(grid_mdp)\n",
    "predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 4, 3, 3)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 4, 3, 32)     416         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 4, 3, 8)      1032        ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 96)           0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 32)           3104        ['flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 16)           528         ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 8)            136         ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 1)            17          ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,233\n",
      "Trainable params: 5,233\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) MCTS conf: -0.31 , det: 0.55 | LTL [---]  LDBA [ 0.0 ] path: [8, 8, 5, 2, 5, 8, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 5, 2, 2, 2]\n",
      "1 ) MCTS conf: 0.02 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "2 ) MCTS conf: 0.98 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [1, 0, 0]\n",
      "3 ) MCTS conf: -0.47 , det: 0.57 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "4 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "5 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "6 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [1, 0, 0]\n",
      "7 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "8 ) MCTS conf: 0.94 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "9 ) MCTS conf: 0.09 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "Train wins: 9 / 10\n"
     ]
    }
   ],
   "source": [
    "LTL_coef = 10\n",
    "NN_value_active = False\n",
    "search_depth = 100\n",
    "MCTS_samples = 100\n",
    "training = True\n",
    "epochs = 15\n",
    "C = 1\n",
    "tow = 0.1\n",
    "T = [25]\n",
    "K = 1\n",
    "batch_size = 32\n",
    "steps_per_epoch = 4\n",
    "idx = 0\n",
    "success_rates = []\n",
    "succes_std = []\n",
    "win_hist = []\n",
    "train_history = []\n",
    "best_val_len = {}\n",
    "for s in csrl.states(): best_val_len[s] = (0.001, 99999)\n",
    "\n",
    "num_training_epochs =  10\n",
    "# os.remove(\"Log_run.txt\")\n",
    "for i in T:\n",
    "    idx += 1\n",
    "    train_wins = 0\n",
    "    # model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "    N, W, Q, P, visited_train = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_training_epochs):\n",
    "        t1 = time.time()\n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=C, tow=tow, n_samples=MCTS_samples, visited=visited_train,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=K, NN_value_active=NN_value_active, run_num=epoch, ltl_f_rew=False, reachability=True, \n",
    "                best_val_len = best_val_len)\n",
    "        \n",
    "        if reward_history[-1]>0:\n",
    "            train_wins+=1\n",
    "            NN_value_active = True\n",
    "\n",
    "        if training and len(action_history)>0:\n",
    "            if epoch==0:\n",
    "                x_train = np.array(channeled_states)[:-1]\n",
    "                y1_train = np.array(better_policy)\n",
    "                y2_train = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                y2_train = y2_train[:-1]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train, np.array(channeled_states)[:-1]),0)\n",
    "                y1_train = np.concatenate((y1_train, np.array(better_policy)),0)\n",
    "                y2_train_curr = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                y2_train = np.concatenate((y2_train, y2_train_curr[:-1]),0)\n",
    "            tr_hist = model.fit(x_train, [y1_train, y2_train], epochs=epochs, batch_size=batch_size,\n",
    "                                steps_per_epoch=steps_per_epoch if len(x_train)>steps_per_epoch*epochs*batch_size else None, verbose=0)\n",
    "            train_history += tr_hist.history['loss']\n",
    "    print(\"Train wins:\",train_wins,\"/\", num_training_epochs)\n",
    "\n",
    "# u, d, r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36059/263196528.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n"
     ]
    }
   ],
   "source": [
    "Policy = np.zeros(csrl.shape)\n",
    "for i in csrl.states():\n",
    "    x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n",
    "    Policy[i] = np.nan_to_num(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_gws.append(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0))\n",
    "policies.append(Policy[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 4., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [4., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 4.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 4., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 4.]])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(input_gws)\n",
    "y = np.array(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4, 3, 3, 8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3965 - accuracy: 0.7639\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3962 - accuracy: 0.7639\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3960 - accuracy: 0.7639\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3958 - accuracy: 0.7685\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3956 - accuracy: 0.7685\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3954 - accuracy: 0.7731\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3952 - accuracy: 0.7778\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3950 - accuracy: 0.7778\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3948 - accuracy: 0.7731\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3946 - accuracy: 0.7731\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3945 - accuracy: 0.7731\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3943 - accuracy: 0.7731\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3941 - accuracy: 0.7731\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3939 - accuracy: 0.7731\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3938 - accuracy: 0.7778\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3936 - accuracy: 0.7731\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3935 - accuracy: 0.7778\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3933 - accuracy: 0.7778\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3932 - accuracy: 0.7778\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3930 - accuracy: 0.7824\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3929 - accuracy: 0.7824\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3927 - accuracy: 0.7778\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3926 - accuracy: 0.7778\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3924 - accuracy: 0.7778\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3923 - accuracy: 0.7731\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3922 - accuracy: 0.7685\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3921 - accuracy: 0.7685\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3919 - accuracy: 0.7685\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3918 - accuracy: 0.7731\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3917 - accuracy: 0.7731\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3916 - accuracy: 0.7731\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3915 - accuracy: 0.7731\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3913 - accuracy: 0.7685\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3912 - accuracy: 0.7685\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3911 - accuracy: 0.7685\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3910 - accuracy: 0.7731\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3909 - accuracy: 0.7778\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3908 - accuracy: 0.7731\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3907 - accuracy: 0.7685\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3906 - accuracy: 0.7685\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3905 - accuracy: 0.7685\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3904 - accuracy: 0.7685\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3903 - accuracy: 0.7685\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3902 - accuracy: 0.7685\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3902 - accuracy: 0.7685\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3901 - accuracy: 0.7685\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3900 - accuracy: 0.7685\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3899 - accuracy: 0.7731\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3898 - accuracy: 0.7731\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3897 - accuracy: 0.7731\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3896 - accuracy: 0.7731\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3896 - accuracy: 0.7731\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3895 - accuracy: 0.7639\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3894 - accuracy: 0.7639\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3893 - accuracy: 0.7593\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3893 - accuracy: 0.7593\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3892 - accuracy: 0.7593\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3891 - accuracy: 0.7593\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3890 - accuracy: 0.7593\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3890 - accuracy: 0.7593\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3889 - accuracy: 0.7639\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3888 - accuracy: 0.7685\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3888 - accuracy: 0.7778\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3887 - accuracy: 0.7778\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3886 - accuracy: 0.7731\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3886 - accuracy: 0.7685\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3885 - accuracy: 0.7685\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3885 - accuracy: 0.7731\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3884 - accuracy: 0.7731\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3883 - accuracy: 0.7685\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3883 - accuracy: 0.7685\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3882 - accuracy: 0.7685\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3882 - accuracy: 0.7639\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3881 - accuracy: 0.7593\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3881 - accuracy: 0.7593\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3880 - accuracy: 0.7685\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3880 - accuracy: 0.7685\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3879 - accuracy: 0.7731\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3878 - accuracy: 0.7685\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3878 - accuracy: 0.7685\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3877 - accuracy: 0.7731\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3877 - accuracy: 0.7685\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3876 - accuracy: 0.7731\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3876 - accuracy: 0.7685\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3876 - accuracy: 0.7685\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3875 - accuracy: 0.7639\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3875 - accuracy: 0.7685\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3874 - accuracy: 0.7685\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3874 - accuracy: 0.7685\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3873 - accuracy: 0.7685\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3873 - accuracy: 0.7685\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3872 - accuracy: 0.7685\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3872 - accuracy: 0.7731\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3871 - accuracy: 0.7731\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3871 - accuracy: 0.7731\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3871 - accuracy: 0.7685\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3870 - accuracy: 0.7685\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3870 - accuracy: 0.7685\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3869 - accuracy: 0.7685\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3869 - accuracy: 0.7639\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3869 - accuracy: 0.7639\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3867 - accuracy: 0.7778\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3867 - accuracy: 0.7778\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3866 - accuracy: 0.7731\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3866 - accuracy: 0.7731\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3866 - accuracy: 0.7685\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3861 - accuracy: 0.7731\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3861 - accuracy: 0.7731\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3861 - accuracy: 0.7685\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3860 - accuracy: 0.7685\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3860 - accuracy: 0.7731\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.7685\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.7731\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3859 - accuracy: 0.7639\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3858 - accuracy: 0.7639\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3858 - accuracy: 0.7639\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3858 - accuracy: 0.7685\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3858 - accuracy: 0.7685\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3857 - accuracy: 0.7731\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3857 - accuracy: 0.7778\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3857 - accuracy: 0.7639\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3857 - accuracy: 0.7593\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3856 - accuracy: 0.7593\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3856 - accuracy: 0.7593\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3856 - accuracy: 0.7639\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3856 - accuracy: 0.7639\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3854 - accuracy: 0.7639\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3853 - accuracy: 0.7500\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3853 - accuracy: 0.7593\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3852 - accuracy: 0.7500\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3852 - accuracy: 0.7546\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3851 - accuracy: 0.7639\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3851 - accuracy: 0.7593\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3851 - accuracy: 0.7546\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3851 - accuracy: 0.7500\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3851 - accuracy: 0.7500\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3850 - accuracy: 0.7546\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3850 - accuracy: 0.7593\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3850 - accuracy: 0.7639\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3849 - accuracy: 0.7593\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3849 - accuracy: 0.7639\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3848 - accuracy: 0.7685\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3848 - accuracy: 0.7593\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3848 - accuracy: 0.7546\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3848 - accuracy: 0.7500\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3847 - accuracy: 0.7500\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3847 - accuracy: 0.7407\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3847 - accuracy: 0.7500\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3847 - accuracy: 0.7407\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3845 - accuracy: 0.7407\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3845 - accuracy: 0.7500\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3845 - accuracy: 0.7546\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3845 - accuracy: 0.7500\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3844 - accuracy: 0.7546\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7546\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3844 - accuracy: 0.7454\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3842 - accuracy: 0.7454\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3842 - accuracy: 0.7500\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3842 - accuracy: 0.7593\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3842 - accuracy: 0.7593\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3842 - accuracy: 0.7500\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3841 - accuracy: 0.7639\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3841 - accuracy: 0.7639\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3840 - accuracy: 0.7546\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3836 - accuracy: 0.7639\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3836 - accuracy: 0.7639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m policy_model\u001b[39m.\u001b[39;49mfit(x, y, epochs \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:1712\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m     }\n\u001b[1;32m   1710\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1712\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1713\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    453\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 454\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:1105\u001b[0m, in \u001b[0;36mProgbarLogger.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_finalize_progbar(logs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:1182\u001b[0m, in \u001b[0;36mProgbarLogger._finalize_progbar\u001b[0;34m(self, logs, counter)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m counter \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen\n\u001b[1;32m   1181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget\n\u001b[0;32m-> 1182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py:297\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    294\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 297\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    298\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message)\n\u001b[0;32m---> 80\u001b[0m     sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mflush()\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/iostream.py:475\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[39msend will happen in the background thread\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\n\u001b[1;32m    470\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mthread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m ):\n\u001b[1;32m    474\u001b[0m     \u001b[39m# request flush on the background thread\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flush)\n\u001b[1;32m    476\u001b[0m     \u001b[39m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     evt \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mEvent()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/iostream.py:210\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[1;32m    209\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/zmq/sugar/socket.py:688\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    681\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[1;32m    682\u001b[0m             data,\n\u001b[1;32m    683\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[1;32m    684\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[1;32m    686\u001b[0m         )\n\u001b[1;32m    687\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[0;32m--> 688\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_model.fit(x, y, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_policy = policy_model(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).reshape(1,3,3))\n",
    "# pred_policy = policy_model(input_gws[4].reshape(1,3,3))\n",
    "\n",
    "pred_policy = np.argmax(pred_policy[0,0], 2)\n",
    "pred_policy = np.where(pred_policy == 0, '^', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '1', 'v', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '2', '>', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '3', '<', pred_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 0., 3.],\n",
       "        [0., 4., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).reshape(1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '>', '6'],\n",
       "       ['>', '^', '^'],\n",
       "       ['^', '^', '<']], dtype='<U21')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 4., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [4., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 4.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 4., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 4.]])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? 0.6\n",
      "0 ) MCTS conf: 0.17 , det: 1.0 | ? 1\n",
      "LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "? 0.05260303\n",
      "? 1\n",
      "1 ) MCTS conf: 0.96 , det: 1.0 | ? 1\n",
      "LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "? 0.06288229\n",
      "? 0.9939205570480697\n",
      "? 1\n",
      "2 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [6, 3, 0, 0]\n",
      "? 0.0014226277\n",
      "? 1\n",
      "3 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "? 0.34072602\n",
      "? 0.37543768\n",
      "? 0.3911557\n",
      "? 1\n",
      "4 ) MCTS conf: 0.22 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "5 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "6 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "7 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "8 ) MCTS conf: 0.19 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "9 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "Train wins: 10 / 10\n",
      "? 0.31173742\n",
      "? 0.32700434\n",
      "? 1\n",
      "None ) MCTS conf: 0.8 , det: 0.7 | LTL [+++]  LDBA [ 0.01 ] path: [1, 0, 0]\n",
      "None ) MCTS conf: 0.96 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 0.18 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 0.97 , det: 0.99 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [6, 3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 0.99 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "Test wins: 10 / 10\n"
     ]
    }
   ],
   "source": [
    "visited_states_train = []\n",
    "visited_states_test = []\n",
    "LTL_coef = 10\n",
    "NN_value_active = False\n",
    "\n",
    "search_depth = 100\n",
    "MCTS_samples = 100\n",
    "\n",
    "num_training_epochs =  10\n",
    "num_test_epochs = 10\n",
    "training = True\n",
    "epochs = 10\n",
    "C = 1\n",
    "tow = 0.1\n",
    "T = [25]\n",
    "K = 1\n",
    "batch_size = 32\n",
    "steps_per_epoch = 4\n",
    "idx = 0\n",
    "success_rates = []\n",
    "succes_std = []\n",
    "win_hist = []\n",
    "train_history = []\n",
    "\n",
    "best_val_len = {}\n",
    "for s in csrl.states(): best_val_len[s] = (0.001, 99999)\n",
    "\n",
    "# os.remove(\"Log_run.txt\")\n",
    "for i in T:\n",
    "    idx += 1\n",
    "    # TRAIN ##############################\n",
    "    train_wins = 0\n",
    "    # num_training_epochs = int(200 - 1.9*i)\n",
    "    # model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "    N, W, Q, P, visited_train = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_training_epochs):\n",
    "        t1 = time.time()\n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=C, tow=tow, n_samples=MCTS_samples, visited=visited_train,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=K, NN_value_active=NN_value_active, run_num=epoch, ltl_f_rew=False, reachability=True, \n",
    "                best_val_len = best_val_len)\n",
    "        # print(\"!\", best_val_len[(0,0,0,0)])\n",
    "        visited_states_train += state_history\n",
    "        t2 = time.time()\n",
    "        # print(t2-t1, \" run episode\")\n",
    "\n",
    "        # win = check_LTL(LTL_formula, trajectory, predicates)[0]\n",
    "        if reward_history[-1]>0:\n",
    "            train_wins+=1\n",
    "            NN_value_active = True\n",
    "\n",
    "        if training and len(action_history)>0:\n",
    "            if epoch==0:\n",
    "                x_train = np.array(channeled_states)[:-1]\n",
    "                y1_train = np.array(better_policy)\n",
    "                y2_train = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                # y2_train = np.array(reward_history)\n",
    "                y2_train = y2_train[:-1]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train, np.array(channeled_states)[:-1]),0)\n",
    "                y1_train = np.concatenate((y1_train, np.array(better_policy)),0)\n",
    "                y2_train_curr = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                # y2_train_curr = np.array(reward_history)\n",
    "                y2_train = np.concatenate((y2_train, y2_train_curr[:-1]),0)\n",
    "            t3= time.time()\n",
    "            # print(t3-t2, \" build database\")\n",
    "            tr_hist = model.fit(x_train, [y1_train, y2_train], epochs=epochs, batch_size=batch_size,\n",
    "                                steps_per_epoch=steps_per_epoch if len(x_train)>steps_per_epoch*epochs*batch_size else None, verbose=0)\n",
    "            train_history += tr_hist.history['loss']\n",
    "        # win_hist.append(win)\n",
    "        t4 = time.time()\n",
    "        # print(t4-t3, \"fit\", len(x_train))\n",
    "    print(\"Train wins:\",train_wins,\"/\", num_training_epochs)\n",
    "\n",
    "    # TEST ##############################\n",
    "    test_wins = 0\n",
    "    N, W, Q, P, visited_test = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_test_epochs):\n",
    "        \n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=1, tow=1, n_samples=MCTS_samples, visited=visited_test,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=1, NN_value_active=True, reachability=True, best_val_len = best_val_len)\n",
    "\n",
    "        # win = check_LTL(LTL_formula, trajectory, predicates)[0]\n",
    "        win = reward_history[-1]\n",
    "        if win: test_wins+=1\n",
    "        win_hist.append(win)\n",
    "        visited_states_test += state_history\n",
    "        \n",
    "    success_rates.append(100*test_wins/num_test_epochs)\n",
    "    temp = np.zeros(num_test_epochs)\n",
    "    temp[:test_wins]=1\n",
    "    std = np.sqrt(num_test_epochs*np.var(temp))\n",
    "    succes_std.append((success_rates[-1]-std, success_rates[-1]+std))\n",
    "    \n",
    "    ###############################################################\n",
    "    print(\"Test wins:\",test_wins,\"/\",num_test_epochs)\n",
    "    # print(\"last reward:\", reward_history[-1], \"  | trajectory:\", trajectory)\n",
    "    # print(\"Actions:\", action_history)\n",
    "\n",
    "encode_visited_states_test = [i[1]*csrl.shape[-2]*csrl.shape[-3]+i[2]*csrl.shape[-2]+i[3] for i in visited_states_test]\n",
    "encode_visited_states_train = [i[1]*csrl.shape[-2]*csrl.shape[-3]+i[2]*csrl.shape[-2]+i[3] for i in visited_states_train]\n",
    "\n",
    "# u, d, r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20664/263196528.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_len ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
