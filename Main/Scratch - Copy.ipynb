{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Absorbing States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:23:40.541992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from dependencies.NN import *\n",
    "from dependencies.LTL import *\n",
    "from dependencies.Utility_funcs import *\n",
    "\n",
    "%matplotlib inline\n",
    "from dependencies.csrl.mdp import GridMDP\n",
    "from dependencies.csrl.oa import OmegaAutomaton\n",
    "from dependencies.csrl import ControlSynthesis\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:23:44.547823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-06 16:23:44.549398: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "input_gws = []\n",
    "policies = []\n",
    "\n",
    "# policy_model = build_policy_model(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).shape, Policy[0,:,:,:].shape)\n",
    "policy_model = build_policy_model((3,3), (4, 3, 3, 8))\n",
    "# policy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Omega-automaton states (including the trap state): 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dependencies.csrl.oa.OmegaAutomaton at 0x7f0fe82f8160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LTL Specification\n",
    "ltl = '(F G a | F G b) & G !d'\n",
    "\n",
    "# Translate the LTL formula to an LDBA\n",
    "oa = OmegaAutomaton(ltl)\n",
    "print('Number of Omega-automaton states (including the trap state):',oa.shape[1])\n",
    "display(oa)\n",
    "\n",
    "# MDP Description\n",
    "shape = (3,3)\n",
    "# E: Empty, T: Trap, B: Obstacle\n",
    "structure = np.array([\n",
    "['E',  'E',  'E'],\n",
    "['E',  'E',  'E'],\n",
    "['E',  'E',  'E'],\n",
    "])\n",
    "\n",
    "# Labels of the states\n",
    "label = np.array([\n",
    "[('a',),(),('b',)],\n",
    "[(),('d',),    ()],\n",
    "[(),    (),    ()],\n",
    "],dtype=object)\n",
    "# Colors of the labels\n",
    "lcmap={\n",
    "    ('a',):'lightgreen',\n",
    "    ('b',):'lightgreen',\n",
    "    ('d',):'pink'\n",
    "}\n",
    "p = 1\n",
    "grid_mdp = GridMDP(shape=shape,structure=structure,label=label,lcmap=lcmap, p=p, figsize=5)  # Use figsize=4 for smaller figures\n",
    "grid_mdp.plot()\n",
    "\n",
    "# Construct the product MDP\n",
    "csrl = ControlSynthesis(grid_mdp,oa)\n",
    "max_rew = round(csrl.reward.max(), 3)\n",
    "\n",
    "s_vectors = state_vectors(csrl)\n",
    "enc = list(np.unique(grid_mdp.label))\n",
    "enc.pop(enc.index(()))\n",
    "ch_states = channeled(csrl, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [0], 'b': [2], 'd': [4]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_t = \"(<> [] a \\/ <> [] b) /\\ [] ~d\"\n",
    "\n",
    "LTL_formula = parser.parse(full_t)\n",
    "predicates=get_predicates(grid_mdp)\n",
    "predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) MCTS conf: -0.36 , det: 1.0 | LTL [---]  LDBA [ 0.0 ] path: [5, 5, 8, 7, 6, 3, 0, 0, 0, 3, 4]\n",
      "1 ) MCTS conf: -0.39 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 7, 6, 3, 0, 0]\n",
      "2 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [6, 3, 0, 0]\n",
      "3 ) MCTS conf: 0.58 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 7, 6, 3, 0, 0]\n",
      "4 ) MCTS conf: 0.96 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [1, 0, 0]\n",
      "5 ) MCTS conf: 0.97 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 7, 6, 3, 0, 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m             y2_train_curr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(reward_history) \u001b[39m+\u001b[39m LTL_coef\u001b[39m*\u001b[39mreward_history[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             y2_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((y2_train, y2_train_curr[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         tr_hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, [y1_train, y2_train], epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                             steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(x_train)\u001b[39m>\u001b[39;49msteps_per_epoch\u001b[39m*\u001b[39;49mepochs\u001b[39m*\u001b[39;49mbatch_size \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         train_history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_hist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/Main/Scratch.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain wins:\u001b[39m\u001b[39m\"\u001b[39m,train_wins,\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, num_training_epochs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LTL_coef = 10\n",
    "NN_value_active = False\n",
    "search_depth = 100\n",
    "MCTS_samples = 100\n",
    "training = True\n",
    "epochs = 15\n",
    "C = 1\n",
    "tow = 0.1\n",
    "T = [25]\n",
    "K = 1\n",
    "batch_size = 32\n",
    "steps_per_epoch = 4\n",
    "idx = 0\n",
    "success_rates = []\n",
    "succes_std = []\n",
    "win_hist = []\n",
    "train_history = []\n",
    "best_val_len = {}\n",
    "for s in csrl.states(): best_val_len[s] = (0.001, 99999)\n",
    "\n",
    "num_training_epochs =  10\n",
    "# os.remove(\"outputs/Log_run.txt\")\n",
    "for i in T:\n",
    "    idx += 1\n",
    "    train_wins = 0\n",
    "    # model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "    N, W, Q, P, visited_train = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_training_epochs):\n",
    "        t1 = time.time()\n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=C, tow=tow, n_samples=MCTS_samples, visited=visited_train,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=K, NN_value_active=NN_value_active, run_num=epoch, ltl_f_rew=False, reachability=True, \n",
    "                best_val_len = best_val_len)\n",
    "        \n",
    "        if reward_history[-1]>0:\n",
    "            train_wins+=1\n",
    "            NN_value_active = True\n",
    "\n",
    "        if training and len(action_history)>0:\n",
    "            if epoch==0:\n",
    "                x_train = np.array(channeled_states)[:-1]\n",
    "                y1_train = np.array(better_policy)\n",
    "                y2_train = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                y2_train = y2_train[:-1]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train, np.array(channeled_states)[:-1]),0)\n",
    "                y1_train = np.concatenate((y1_train, np.array(better_policy)),0)\n",
    "                y2_train_curr = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                y2_train = np.concatenate((y2_train, y2_train_curr[:-1]),0)\n",
    "            tr_hist = model.fit(x_train, [y1_train, y2_train], epochs=epochs, batch_size=batch_size,\n",
    "                                steps_per_epoch=steps_per_epoch if len(x_train)>steps_per_epoch*epochs*batch_size else None, verbose=0)\n",
    "            train_history += tr_hist.history['loss']\n",
    "    print(\"Train wins:\",train_wins,\"/\", num_training_epochs)\n",
    "\n",
    "# u, d, r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110381/263196528.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n"
     ]
    }
   ],
   "source": [
    "Policy = np.zeros(csrl.shape)\n",
    "for i in csrl.states():\n",
    "    x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n",
    "    Policy[i] = np.nan_to_num(x)\n",
    "\n",
    "input_gws.append(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0))\n",
    "policies.append(Policy[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(input_gws)\n",
    "y = np.array(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3965 - accuracy: 0.7639\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3962 - accuracy: 0.7639\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3960 - accuracy: 0.7639\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3958 - accuracy: 0.7685\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3956 - accuracy: 0.7685\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3954 - accuracy: 0.7731\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3952 - accuracy: 0.7778\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3950 - accuracy: 0.7778\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3948 - accuracy: 0.7731\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3946 - accuracy: 0.7731\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3945 - accuracy: 0.7731\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3943 - accuracy: 0.7731\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3941 - accuracy: 0.7731\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3939 - accuracy: 0.7731\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3938 - accuracy: 0.7778\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3936 - accuracy: 0.7731\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3935 - accuracy: 0.7778\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3933 - accuracy: 0.7778\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3932 - accuracy: 0.7778\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3930 - accuracy: 0.7824\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3929 - accuracy: 0.7824\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3927 - accuracy: 0.7778\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3926 - accuracy: 0.7778\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3924 - accuracy: 0.7778\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3923 - accuracy: 0.7731\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3922 - accuracy: 0.7685\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3921 - accuracy: 0.7685\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3919 - accuracy: 0.7685\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3918 - accuracy: 0.7731\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3917 - accuracy: 0.7731\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3916 - accuracy: 0.7731\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3915 - accuracy: 0.7731\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3913 - accuracy: 0.7685\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.3912 - accuracy: 0.7685\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3911 - accuracy: 0.7685\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3910 - accuracy: 0.7731\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3909 - accuracy: 0.7778\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3908 - accuracy: 0.7731\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3907 - accuracy: 0.7685\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3906 - accuracy: 0.7685\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3905 - accuracy: 0.7685\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3904 - accuracy: 0.7685\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3903 - accuracy: 0.7685\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3902 - accuracy: 0.7685\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3902 - accuracy: 0.7685\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3901 - accuracy: 0.7685\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3900 - accuracy: 0.7685\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3899 - accuracy: 0.7731\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3898 - accuracy: 0.7731\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3897 - accuracy: 0.7731\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3896 - accuracy: 0.7731\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3896 - accuracy: 0.7731\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3895 - accuracy: 0.7639\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3894 - accuracy: 0.7639\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3893 - accuracy: 0.7593\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3893 - accuracy: 0.7593\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3892 - accuracy: 0.7593\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3891 - accuracy: 0.7593\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3890 - accuracy: 0.7593\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3890 - accuracy: 0.7593\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3889 - accuracy: 0.7639\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3888 - accuracy: 0.7685\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3888 - accuracy: 0.7778\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3887 - accuracy: 0.7778\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3886 - accuracy: 0.7731\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3886 - accuracy: 0.7685\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3885 - accuracy: 0.7685\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3885 - accuracy: 0.7731\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3884 - accuracy: 0.7731\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3883 - accuracy: 0.7685\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3883 - accuracy: 0.7685\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3882 - accuracy: 0.7685\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3882 - accuracy: 0.7639\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3881 - accuracy: 0.7593\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3881 - accuracy: 0.7593\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3880 - accuracy: 0.7685\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3880 - accuracy: 0.7685\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3879 - accuracy: 0.7731\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3878 - accuracy: 0.7685\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3878 - accuracy: 0.7685\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3877 - accuracy: 0.7731\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3877 - accuracy: 0.7685\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3876 - accuracy: 0.7731\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3876 - accuracy: 0.7685\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3876 - accuracy: 0.7685\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3875 - accuracy: 0.7639\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3875 - accuracy: 0.7685\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3874 - accuracy: 0.7685\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3874 - accuracy: 0.7685\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3873 - accuracy: 0.7685\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3873 - accuracy: 0.7685\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3872 - accuracy: 0.7685\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3872 - accuracy: 0.7731\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3871 - accuracy: 0.7731\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3871 - accuracy: 0.7731\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3871 - accuracy: 0.7685\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3870 - accuracy: 0.7685\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3870 - accuracy: 0.7685\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3869 - accuracy: 0.7685\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3869 - accuracy: 0.7639\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3869 - accuracy: 0.7639\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3868 - accuracy: 0.7731\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3867 - accuracy: 0.7778\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3867 - accuracy: 0.7778\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3866 - accuracy: 0.7731\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3866 - accuracy: 0.7731\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3866 - accuracy: 0.7685\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3865 - accuracy: 0.7685\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3864 - accuracy: 0.7685\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3863 - accuracy: 0.7685\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3862 - accuracy: 0.7685\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3861 - accuracy: 0.7731\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3861 - accuracy: 0.7731\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3861 - accuracy: 0.7685\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3860 - accuracy: 0.7685\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3860 - accuracy: 0.7731\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.7685\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3860 - accuracy: 0.7731\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3859 - accuracy: 0.7685\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3859 - accuracy: 0.7639\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3858 - accuracy: 0.7639\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3858 - accuracy: 0.7639\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3858 - accuracy: 0.7685\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3858 - accuracy: 0.7685\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3857 - accuracy: 0.7731\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3857 - accuracy: 0.7778\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3857 - accuracy: 0.7639\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3857 - accuracy: 0.7593\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3856 - accuracy: 0.7593\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3856 - accuracy: 0.7593\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3856 - accuracy: 0.7639\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3856 - accuracy: 0.7639\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3855 - accuracy: 0.7593\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3854 - accuracy: 0.7639\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.3854 - accuracy: 0.7593\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3853 - accuracy: 0.7500\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3853 - accuracy: 0.7593\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3853 - accuracy: 0.7546\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3852 - accuracy: 0.7500\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3852 - accuracy: 0.7546\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3852 - accuracy: 0.7593\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3851 - accuracy: 0.7639\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3851 - accuracy: 0.7593\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3851 - accuracy: 0.7546\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3851 - accuracy: 0.7500\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3851 - accuracy: 0.7500\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3850 - accuracy: 0.7546\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3850 - accuracy: 0.7593\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3850 - accuracy: 0.7639\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3850 - accuracy: 0.7500\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3849 - accuracy: 0.7546\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3849 - accuracy: 0.7593\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3849 - accuracy: 0.7639\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3848 - accuracy: 0.7685\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3848 - accuracy: 0.7593\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3848 - accuracy: 0.7546\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3848 - accuracy: 0.7500\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3848 - accuracy: 0.7454\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3847 - accuracy: 0.7500\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3847 - accuracy: 0.7407\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3847 - accuracy: 0.7500\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3847 - accuracy: 0.7454\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3847 - accuracy: 0.7407\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3846 - accuracy: 0.7454\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3845 - accuracy: 0.7454\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3845 - accuracy: 0.7407\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3845 - accuracy: 0.7500\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3845 - accuracy: 0.7546\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3845 - accuracy: 0.7500\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3844 - accuracy: 0.7546\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7546\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3844 - accuracy: 0.7500\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3844 - accuracy: 0.7454\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3843 - accuracy: 0.7500\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3843 - accuracy: 0.7454\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3842 - accuracy: 0.7454\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3842 - accuracy: 0.7500\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3842 - accuracy: 0.7593\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3842 - accuracy: 0.7593\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3842 - accuracy: 0.7500\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3842 - accuracy: 0.7546\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3841 - accuracy: 0.7639\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3841 - accuracy: 0.7639\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3841 - accuracy: 0.7546\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3841 - accuracy: 0.7593\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3840 - accuracy: 0.7546\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3840 - accuracy: 0.7639\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3840 - accuracy: 0.7593\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3839 - accuracy: 0.7593\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3839 - accuracy: 0.7639\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3838 - accuracy: 0.7639\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3838 - accuracy: 0.7593\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3838 - accuracy: 0.7546\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3837 - accuracy: 0.7593\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3837 - accuracy: 0.7639\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3836 - accuracy: 0.7639\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3836 - accuracy: 0.7593\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3836 - accuracy: 0.7685\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3836 - accuracy: 0.7639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/srmt/Research/Uwaterloo/RL-LTL/ComputeCanada_scrips/Scratch.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m policy_model\u001b[39m.\u001b[39;49mfit(x, y, epochs \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:1712\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m     }\n\u001b[1;32m   1710\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1712\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1713\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    453\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 454\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:1105\u001b[0m, in \u001b[0;36mProgbarLogger.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1105\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_finalize_progbar(logs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/callbacks.py:1182\u001b[0m, in \u001b[0;36mProgbarLogger._finalize_progbar\u001b[0;34m(self, logs, counter)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m counter \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen\n\u001b[1;32m   1181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget\n\u001b[0;32m-> 1182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py:297\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    294\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 297\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    298\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/keras/utils/io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message)\n\u001b[0;32m---> 80\u001b[0m     sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mflush()\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/iostream.py:475\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[39msend will happen in the background thread\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\n\u001b[1;32m    470\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mthread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m ):\n\u001b[1;32m    474\u001b[0m     \u001b[39m# request flush on the background thread\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flush)\n\u001b[1;32m    476\u001b[0m     \u001b[39m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     evt \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mEvent()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/iostream.py:210\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[1;32m    209\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/zmq/sugar/socket.py:688\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    681\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[1;32m    682\u001b[0m             data,\n\u001b[1;32m    683\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[1;32m    684\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[1;32m    686\u001b[0m         )\n\u001b[1;32m    687\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[0;32m--> 688\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_model.fit(x, y, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_policy = policy_model(channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).reshape(1,3,3))\n",
    "# pred_policy = policy_model(input_gws[4].reshape(1,3,3))\n",
    "\n",
    "pred_policy = np.argmax(pred_policy[0,0], 2)\n",
    "pred_policy = np.where(pred_policy == 0, '^', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '1', 'v', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '2', '>', pred_policy)\n",
    "pred_policy = np.where(pred_policy == '3', '<', pred_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 0., 3.],\n",
       "        [0., 4., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channeled(csrl, enc, agent=False)[0,0,0,0].sum(0).reshape(1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '>', '6'],\n",
       "       ['>', '^', '^'],\n",
       "       ['^', '^', '<']], dtype='<U21')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 4., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [4., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 4.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 4., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[2., 0., 3.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 4.]])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? 0.6\n",
      "0 ) MCTS conf: 0.17 , det: 1.0 | ? 1\n",
      "LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "? 0.05260303\n",
      "? 1\n",
      "1 ) MCTS conf: 0.96 , det: 1.0 | ? 1\n",
      "LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "? 0.06288229\n",
      "? 0.9939205570480697\n",
      "? 1\n",
      "2 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [6, 3, 0, 0]\n",
      "? 0.0014226277\n",
      "? 1\n",
      "3 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "? 0.34072602\n",
      "? 0.37543768\n",
      "? 0.3911557\n",
      "? 1\n",
      "4 ) MCTS conf: 0.22 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "5 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "6 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "7 ) MCTS conf: 0.99 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "8 ) MCTS conf: 0.19 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "9 ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "Train wins: 10 / 10\n",
      "? 0.31173742\n",
      "? 0.32700434\n",
      "? 1\n",
      "None ) MCTS conf: 0.8 , det: 0.7 | LTL [+++]  LDBA [ 0.01 ] path: [1, 0, 0]\n",
      "None ) MCTS conf: 0.96 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 0.18 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 0.97 , det: 0.99 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [6, 3, 0, 0]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 1.0 | LTL [+++]  LDBA [ 0.01 ] path: [8, 5, 2, 2]\n",
      "None ) MCTS conf: 1.0 , det: 0.99 | LTL [+++]  LDBA [ 0.01 ] path: [7, 6, 3, 0, 0]\n",
      "Test wins: 10 / 10\n"
     ]
    }
   ],
   "source": [
    "visited_states_train = []\n",
    "visited_states_test = []\n",
    "LTL_coef = 10\n",
    "NN_value_active = False\n",
    "\n",
    "search_depth = 100\n",
    "MCTS_samples = 100\n",
    "\n",
    "num_training_epochs =  10\n",
    "num_test_epochs = 10\n",
    "training = True\n",
    "epochs = 10\n",
    "C = 1\n",
    "tow = 0.1\n",
    "T = [25]\n",
    "K = 1\n",
    "batch_size = 32\n",
    "steps_per_epoch = 4\n",
    "idx = 0\n",
    "success_rates = []\n",
    "succes_std = []\n",
    "win_hist = []\n",
    "train_history = []\n",
    "\n",
    "best_val_len = {}\n",
    "for s in csrl.states(): best_val_len[s] = (0.001, 99999)\n",
    "\n",
    "# os.remove(\"Log_run.txt\")\n",
    "for i in T:\n",
    "    idx += 1\n",
    "    # TRAIN ##############################\n",
    "    train_wins = 0\n",
    "    # num_training_epochs = int(200 - 1.9*i)\n",
    "    # model = build_model(ch_states[(0,0,0,0)].shape, csrl.shape[-1])\n",
    "    N, W, Q, P, visited_train = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_training_epochs):\n",
    "        t1 = time.time()\n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=C, tow=tow, n_samples=MCTS_samples, visited=visited_train,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=K, NN_value_active=NN_value_active, run_num=epoch, ltl_f_rew=False, reachability=True, \n",
    "                best_val_len = best_val_len)\n",
    "        # print(\"!\", best_val_len[(0,0,0,0)])\n",
    "        visited_states_train += state_history\n",
    "        t2 = time.time()\n",
    "        # print(t2-t1, \" run episode\")\n",
    "\n",
    "        # win = check_LTL(LTL_formula, trajectory, predicates)[0]\n",
    "        if reward_history[-1]>0:\n",
    "            train_wins+=1\n",
    "            NN_value_active = True\n",
    "\n",
    "        if training and len(action_history)>0:\n",
    "            if epoch==0:\n",
    "                x_train = np.array(channeled_states)[:-1]\n",
    "                y1_train = np.array(better_policy)\n",
    "                y2_train = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                # y2_train = np.array(reward_history)\n",
    "                y2_train = y2_train[:-1]\n",
    "            else:\n",
    "                x_train = np.concatenate((x_train, np.array(channeled_states)[:-1]),0)\n",
    "                y1_train = np.concatenate((y1_train, np.array(better_policy)),0)\n",
    "                y2_train_curr = np.array(reward_history) + LTL_coef*reward_history[-1]\n",
    "                # y2_train_curr = np.array(reward_history)\n",
    "                y2_train = np.concatenate((y2_train, y2_train_curr[:-1]),0)\n",
    "            t3= time.time()\n",
    "            # print(t3-t2, \" build database\")\n",
    "            tr_hist = model.fit(x_train, [y1_train, y2_train], epochs=epochs, batch_size=batch_size,\n",
    "                                steps_per_epoch=steps_per_epoch if len(x_train)>steps_per_epoch*epochs*batch_size else None, verbose=0)\n",
    "            train_history += tr_hist.history['loss']\n",
    "        # win_hist.append(win)\n",
    "        t4 = time.time()\n",
    "        # print(t4-t3, \"fit\", len(x_train))\n",
    "    print(\"Train wins:\",train_wins,\"/\", num_training_epochs)\n",
    "\n",
    "    # TEST ##############################\n",
    "    test_wins = 0\n",
    "    N, W, Q, P, visited_test = np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), np.zeros(csrl.shape), set()\n",
    "    for epoch in range(num_test_epochs):\n",
    "        \n",
    "        state_history, channeled_states, trajectory, action_history, reward_history, better_policy, best_val_len = MC_learning(csrl, model, LTL_formula,\n",
    "                predicates, csrl.reward, ch_states, N = N, W = W, Q = Q, P = P, C=1, tow=1, n_samples=MCTS_samples, visited=visited_test,\n",
    "                start=None, search_depth=search_depth, verbose=0, T=i, K=1, NN_value_active=True, reachability=True, best_val_len = best_val_len)\n",
    "\n",
    "        # win = check_LTL(LTL_formula, trajectory, predicates)[0]\n",
    "        win = reward_history[-1]\n",
    "        if win: test_wins+=1\n",
    "        win_hist.append(win)\n",
    "        visited_states_test += state_history\n",
    "        \n",
    "    success_rates.append(100*test_wins/num_test_epochs)\n",
    "    temp = np.zeros(num_test_epochs)\n",
    "    temp[:test_wins]=1\n",
    "    std = np.sqrt(num_test_epochs*np.var(temp))\n",
    "    succes_std.append((success_rates[-1]-std, success_rates[-1]+std))\n",
    "    \n",
    "    ###############################################################\n",
    "    print(\"Test wins:\",test_wins,\"/\",num_test_epochs)\n",
    "    # print(\"last reward:\", reward_history[-1], \"  | trajectory:\", trajectory)\n",
    "    # print(\"Actions:\", action_history)\n",
    "\n",
    "encode_visited_states_test = [i[1]*csrl.shape[-2]*csrl.shape[-3]+i[2]*csrl.shape[-2]+i[3] for i in visited_states_test]\n",
    "encode_visited_states_train = [i[1]*csrl.shape[-2]*csrl.shape[-3]+i[2]*csrl.shape[-2]+i[3] for i in visited_states_train]\n",
    "\n",
    "# u, d, r, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20664/263196528.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  x = (N[i]**(1/tow)) / np.sum(N[i]**(1/tow))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_len ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
